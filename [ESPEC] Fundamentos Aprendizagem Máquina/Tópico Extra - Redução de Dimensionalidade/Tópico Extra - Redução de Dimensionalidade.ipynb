{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "77f91b98-dc9a-4e88-85fc-37582f9818e2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Redução de Dimensionalidade - Prof André Hochuli"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ed5412de-1e66-434b-9a49-e5b4f306636c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Neste notebook exploraremos técnicas de redução de dimensionalidade, com foco em PCA (Principal Component Analysis) e t-SNE (t-Distributed Stochastic Neighbor Embedding).\n",
    "\n",
    "Em cenários reais de Machine Learning, os dados frequentemente apresentam alta dimensionalidade, seja devido a grande número de atributos, vetores de características extraídos por CNNs ou embeddings latentes produzidos por autoencoders. Trabalhar diretamente nesses espaços pode:\n",
    "\n",
    "* Aumentar custo computacional\n",
    "\n",
    "* Introduzir redundância e correlação entre variáveis\n",
    "\n",
    "* Dificultar análise exploratória e interpretação\n",
    "\n",
    "* Sofrer com a maldição da dimensionalidade\n",
    "\n",
    "**Objetivos desta notebook**\n",
    "\n",
    "* Compreender o papel da redução dimensional no pipeline de ML\n",
    "\n",
    "* Aplicar PCA como técnica linear baseada em decomposição espectral\n",
    "\n",
    "* Aplicar t-SNE como técnica não linear voltada à preservação de estruturas locais\n",
    "\n",
    "* Comparar visualmente os métodos em projeções bidimensionais\n",
    "\n",
    "**Visão conceitual**\n",
    "\n",
    "PCA: projeta os dados em direções ortogonais de máxima variância (autovetores da matriz de covariância), sendo útil para compressão, remoção de redundância e pré-processamento.\n",
    "\n",
    "t-SNE: preserva vizinhanças locais ao minimizar a divergência KL entre distribuições de similaridade, sendo mais indicado para visualização e análise qualitativa de separabilidade de classes.\n",
    "\n",
    "Ao longo da notebook, analisaremos os resultados tanto sob a perspectiva matemática quanto sob a ótica prática de análise exploratória de embeddings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9198c5ea-0cdd-4284-9ca6-fb115513827b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3e27d7d8-b324-4169-99f7-5b24ba66094e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import plotly.graph_objects as go\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as mcolors\n",
    "\n",
    "from sklearn.datasets import load_wine,load_breast_cancer,load_digits,make_classification, load_iris\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import classification_report, accuracy_score, ConfusionMatrixDisplay\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e9c2bfeb-98e8-4f04-aeba-376ee476e651",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#Dataset Sintético"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d6eb3aa9-cd8f-48bb-b546-7b51a9b62581",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ----------------------------\n",
    "# Configuração do problema (sintético)\n",
    "# ----------------------------\n",
    "n_samples = 1000\n",
    "n_features = 300\n",
    "n_informative = 200      # Sinal real\n",
    "n_redundant = 30        # Colinearidade\n",
    "n_repeated = 0\n",
    "n_classes = 3\n",
    "\n",
    "synthetic = make_classification(\n",
    "    n_samples=n_samples,\n",
    "    n_features=n_features,\n",
    "    n_informative=n_informative,\n",
    "    n_redundant=n_redundant,\n",
    "    n_repeated=n_repeated,\n",
    "    n_classes=n_classes,\n",
    "    n_clusters_per_class=2,\n",
    "    class_sep=1.5,\n",
    "    flip_y=0.01,\n",
    "    random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7ff32ac1-71f7-479c-b7a6-00ed3fe40c4c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#Carregando o dataset e normalizando"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cec98f43-f7ff-44a6-bb39-92a4687fc333",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#X, y = load_wine(return_X_y=True)  \n",
    "#X, y = load_iris(return_X_y=True)\n",
    "#X, y = load_breast_cancer(return_X_y=True)  \n",
    "X, y = load_digits(return_X_y=True)  \n",
    "#X,y = synthetic\n",
    "\n",
    "\n",
    "\n",
    "print(X.shape,y.shape)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_scaled, y, test_size=0.25, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "components = np.arange(1, X_scaled.shape[1] + 1)\n",
    "\n",
    "# Número de amostras\n",
    "n_samples = X.shape[0]\n",
    "\n",
    "# Número de features\n",
    "n_features = X.shape[1]\n",
    "\n",
    "# Classes únicas\n",
    "classes = np.unique(y)\n",
    "\n",
    "# Número de classes\n",
    "n_classes = len(classes)\n",
    "\n",
    "print(f\"Número de amostras: {n_samples}\")\n",
    "print(f\"Número de features: {n_features}\")\n",
    "print(f\"Classes: {classes}\")\n",
    "print(f\"Número de classes: {n_classes}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c5c6fc7b-d8ee-4c05-abaf-5ebf65303817",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#PCA "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1cf53e8e-654e-4d4e-8db3-deec176247ba",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Principal Component Analysis (PCA)\n",
    "\n",
    "O **PCA (Principal Component Analysis)** é uma técnica clássica de redução de dimensionalidade baseada em álgebra linear, cujo objetivo é projetar os dados em um novo sistema de coordenadas ortogonais que maximize a variância explicada.\n",
    "\n",
    "Formalmente, o método parte da **matriz de covariância** dos dados padronizados e realiza sua decomposição espectral. Os **autovetores** definem as direções dos componentes principais, enquanto os **autovalores** quantificam a variância associada a cada componente. Os componentes são ordenados de forma decrescente pela variância explicada, permitindo selecionar apenas os mais informativos.\n",
    "\n",
    "Do ponto de vista geométrico, o PCA realiza uma projeção linear no subespaço de maior energia, preservando ao máximo a dispersão global dos dados. Essa propriedade torna o método adequado para:\n",
    "\n",
    "* Compressão de dados\n",
    "* Remoção de redundância e correlação\n",
    "* Redução de ruído\n",
    "* Pré-processamento para classificadores\n",
    "\n",
    "Além disso, a análise da **variância acumulada** fornece um critério objetivo para determinar o número mínimo de componentes necessários para representar adequadamente a estrutura do conjunto de dados.\n",
    "__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "272d3fcf-b685-4e22-b303-cb3047f22f33",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##Inicialização"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "52dd3625-f357-42c1-bb88-b6d1e8fcf867",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "pca = PCA()\n",
    "pca.fit(X_scaled)\n",
    "explained_variance = pca.explained_variance_ratio_\n",
    "cumulative_variance = np.cumsum(explained_variance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "54a1d5cf-8e95-47dd-aa2b-c8cffc52ec0b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Variância Explicada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "35b037c1-b9d6-43af-88f9-c374c9213831",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "# Barras - variância individual\n",
    "plt.bar(components, explained_variance, alpha=0.7)\n",
    "\n",
    "# Linha - variância acumulada\n",
    "plt.plot(components, cumulative_variance, marker='o')\n",
    "\n",
    "plt.xlabel(\"Número de Componentes Principais\")\n",
    "plt.ylabel(\"Variância Explicada\")\n",
    "plt.title(\"PCA - Variância Individual e Acumulada\")\n",
    "plt.xticks(components)\n",
    "plt.ylim(0, 1.05)\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# ----------------------------\n",
    "# Seleção (ex: 95%)\n",
    "# ----------------------------\n",
    "n_95 = np.argmax(cumulative_variance >= 0.95) + 1\n",
    "print(f\"Número mínimo de componentes para ≥95% da variância: {n_95}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d1929784-cef4-4fcc-8a42-818dd968c889",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "for i, var in enumerate(cumulative_variance):\n",
    "    print(f\"{i+1} componentes -> {var:.4f} variância acumulada\")\n",
    "\n",
    "# Número mínimo de componentes para 95% de variância\n",
    "n_95 = np.argmax(cumulative_variance >= 0.95) + 1\n",
    "print(f\"\\nNúmero mínimo de componentes para ≥95% da variância: {n_95}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b113ef07-267a-4599-a4c0-c175cf66c707",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Visualização do Espaço (2D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "07012389-c7bb-4757-9030-438dcf260ff3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "pca_2d = PCA(n_components=2)\n",
    "X_pca_2d = pca_2d.fit_transform(X_scaled)\n",
    "\n",
    "df_2d = pd.DataFrame({\n",
    "    \"PC1\": X_pca_2d[:, 0],\n",
    "    \"PC2\": X_pca_2d[:, 1],\n",
    "    \"Class\": y\n",
    "})\n",
    "\n",
    "# ----------------------------\n",
    "# Plot 2D com Seaborn\n",
    "# ----------------------------\n",
    "plt.figure()\n",
    "sns.scatterplot(\n",
    "    data=df_2d,\n",
    "    x=\"PC1\",\n",
    "    y=\"PC2\",\n",
    "    hue=\"Class\",\n",
    "    palette=\"deep\"\n",
    ")\n",
    "\n",
    "plt.title(\"PCA - Projeção 2D\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Variância explicada (2D):\",\n",
    "      np.sum(pca_2d.explained_variance_ratio_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f230d5c5-71b0-42d7-9c9f-93f49ac5edb2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##Visualização do Espaço (3D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7bbde0de-f5e1-4192-b4a3-16693e9950b2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ----------------------------\n",
    "# PCA 3D\n",
    "# ----------------------------\n",
    "pca_3d = PCA(n_components=3)\n",
    "X_pca_3d = pca_3d.fit_transform(X_scaled)\n",
    "\n",
    "df_3d = pd.DataFrame({\n",
    "    \"PC1\": X_pca_3d[:, 0],\n",
    "    \"PC2\": X_pca_3d[:, 1],\n",
    "    \"PC3\": X_pca_3d[:, 2],\n",
    "    \"Class\": y.astype(str)\n",
    "})\n",
    "\n",
    "# ----------------------------\n",
    "# Paleta Seaborn\n",
    "# ----------------------------\n",
    "classes = df_3d[\"Class\"].unique()\n",
    "palette = sns.color_palette(\"deep\", len(classes))\n",
    "palette_hex = [mcolors.to_hex(c) for c in palette]\n",
    "color_map = {cls: palette_hex[i] for i, cls in enumerate(classes)}\n",
    "\n",
    "# ----------------------------\n",
    "# Figura interativa\n",
    "# ----------------------------\n",
    "fig = go.Figure()\n",
    "\n",
    "for cls in classes:\n",
    "    subset = df_3d[df_3d[\"Class\"] == cls]\n",
    "\n",
    "    fig.add_trace(go.Scatter3d(\n",
    "        x=subset[\"PC1\"],\n",
    "        y=subset[\"PC2\"],\n",
    "        z=subset[\"PC3\"],\n",
    "        mode='markers',\n",
    "        marker=dict(\n",
    "            size=4,\n",
    "            color=color_map[cls],\n",
    "            opacity=0.8\n",
    "        ),\n",
    "        name=f\"Class {cls}\"\n",
    "    ))\n",
    "\n",
    "fig.update_layout(\n",
    "    title=f\"PCA 3D Interativo (Var acumulada = {pca_3d.explained_variance_ratio_.sum():.3f})\",\n",
    "    template=\"plotly_white\",\n",
    "    paper_bgcolor=\"white\",\n",
    "    scene=dict(\n",
    "        bgcolor=\"white\",\n",
    "        xaxis_title=\"PC1\",\n",
    "        yaxis_title=\"PC2\",\n",
    "        zaxis_title=\"PC3\"\n",
    "    ),\n",
    "    legend=dict(itemsizing='constant')\n",
    ")\n",
    "\n",
    "fig.show()\n",
    "print(\"Variância explicada (3D):\",\n",
    "      np.sum(pca_3d.explained_variance_ratio_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "85dc3c84-37d3-4680-a25f-e8644ad3435a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Impacto da Variância Explicada (Componentes do PCA) na Classificação\n",
    "\n",
    "\n",
    "\n",
    "A análise do impacto do número de componentes principais na performance dos classificadores permite avaliar o trade-off entre **compressão de informação** e **capacidade discriminativa**. No PCA, cada componente principal é ordenado pela variância explicada; entretanto, maior variância não implica necessariamente maior relevância para separação de classes.\n",
    "\n",
    "Neste experimento, avaliou-se o desempenho de **SVC**, **KNN**, **Random Forest** e **Decision Tree** à medida que o número de componentes foi progressivamente alterado. O objetivo foi observar como a retenção de diferentes níveis de variância acumulada influencia a acurácia e a generalização dos modelos.\n",
    "\n",
    "Os resultados evidenciam três comportamentos típicos:\n",
    "\n",
    "1. **Região de sub-representação** (poucos componentes): perda de informação discriminativa, afetando principalmente modelos baseados em distância, como KNN e SVC.\n",
    "2. **Região ótima intermediária**: número reduzido de componentes preserva estrutura relevante e reduz ruído, podendo inclusive melhorar generalização.\n",
    "3. **Região de saturação**: aumento marginal de desempenho ao incluir componentes de baixa variância, que tendem a capturar ruído ou variações pouco informativas.\n",
    "\n",
    "Observa-se ainda que:\n",
    "\n",
    "* **SVC** tende a se beneficiar de uma representação compacta e bem estruturada.\n",
    "* **KNN** é sensível à geometria do espaço projetado.\n",
    "* **Random Forest** e **Decision Tree** são menos sensíveis à correlação original, mas podem se beneficiar da redução de redundância.\n",
    "\n",
    "Assim, a variância acumulada deve ser interpretada não apenas como critério estatístico, mas como variável experimental no contexto supervisionado, sendo recomendável validar empiricamente o número ótimo de componentes para cada modelo.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b0232515-cb53-4b27-a536-0ad32fff622d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# ----------------------------\n",
    "# Função: treina e avalia para um alvo de variância\n",
    "# ----------------------------\n",
    "def train_eval(model_name, clf, var_target):\n",
    "    if var_target == 1.0:\n",
    "        pca = PCA(n_components=None)\n",
    "        var_label = \"100%\"\n",
    "    else:\n",
    "        pca = PCA(n_components=var_target, svd_solver=\"full\")\n",
    "        var_label = f\"{int(var_target*100)}%\"\n",
    "\n",
    "    pipe = Pipeline([\n",
    "        (\"scaler\", StandardScaler()),\n",
    "        (\"pca\", pca),\n",
    "        (\"clf\", clf)\n",
    "    ])\n",
    "\n",
    "    pipe.fit(X_train, y_train)\n",
    "    y_pred = pipe.predict(X_test)\n",
    "\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    n_comp = pipe.named_steps[\"pca\"].n_components_\n",
    "\n",
    "    print(f\"[{model_name}] var={int(var_target*100)}% | n_comp={n_comp:2d}/{X.shape[1]} | acc={acc:.4f}\")\n",
    "\n",
    "# ----------------------------\n",
    "# Modelos (valores default razoáveis para demo)\n",
    "# ----------------------------\n",
    "models = {\n",
    "    \"SVM\": SVC(kernel=\"rbf\", C=10, gamma=\"scale\", random_state=42),\n",
    "    \"KNN\": KNeighborsClassifier(n_neighbors=7, weights=\"distance\", p=2),\n",
    "    \"DecisionTree\": DecisionTreeClassifier(random_state=42, max_depth=None),\n",
    "    \"RandomForest\": RandomForestClassifier(\n",
    "        n_estimators=300, random_state=42, n_jobs=-1, max_features=\"sqrt\"\n",
    "    )\n",
    "}\n",
    "\n",
    "# ----------------------------\n",
    "# Rodar para 100%, 95%, 80%...\n",
    "# ----------------------------\n",
    "for name, clf in models.items():\n",
    "    print(f\"\\n========== {name} ===========\")\n",
    "    for var_target in [1.00, 0.95, 0.80,.50,.30]:\n",
    "        train_eval(name, clf, var_target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0f7722e4-b31a-4b85-9ffe-aa9ed246a0aa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#TSNE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "75a68d43-308e-4837-b35e-b5776a0d3186",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##Estrutura Local e Estabilidade"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b8f72b19-5d73-487b-86ee-c8a447bbbc8d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Nesta etapa, utilizamos o **t-SNE** como ferramenta de visualização para investigar a preservação da **estrutura local** do espaço de características. Diferentemente do PCA, que realiza uma projeção linear orientada à variância global, o t-SNE modela probabilidades de vizinhança no espaço original e busca preservar essas relações no espaço reduzido por meio da minimização da divergência de Kullback–Leibler.\n",
    "\n",
    "O foco desta análise não é desempenho classificatório, mas sim **qualidade estrutural da organização dos dados** em 2D, especialmente a formação de clusters e a separação qualitativa entre classes.\n",
    "\n",
    "Para evidenciar uma propriedade importante do método, realizamos **seis execuções independentes**, variando a inicialização aleatória. Como o t-SNE envolve otimização não convexa, os resultados podem apresentar variações significativas entre execuções, mesmo mantendo os mesmos hiperparâmetros (perplexity, learning rate, número de iterações). Essa instabilidade se manifesta principalmente:\n",
    "\n",
    "* Na orientação global dos clusters\n",
    "* Na distância relativa entre grupos\n",
    "* Na forma geométrica das regiões projetadas\n",
    "\n",
    "Entretanto, observa-se que a **estrutura local intra-cluster tende a ser preservada**, reforçando o caráter exploratório da técnica.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "91889c02-67b6-412e-a1a0-8f194b8ef3bb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "y_str = y.astype(str)\n",
    "\n",
    "# ----------------------------\n",
    "# Configuração t-SNE \n",
    "# ----------------------------\n",
    "perplexity = 30\n",
    "n_iter = 1000\n",
    "learning_rate = \"auto\"\n",
    "init = \"random\" \n",
    "\n",
    "seeds = [0, 1, 2, 3, 4, 5]  # 6 execuções\n",
    "\n",
    "# ----------------------------\n",
    "# Plot grid (2x3)\n",
    "# ----------------------------\n",
    "fig, axes = plt.subplots(2, 3, figsize=(12, 8))\n",
    "axes = axes.ravel()\n",
    "\n",
    "for ax, seed in zip(axes, seeds):\n",
    "    tsne = TSNE(\n",
    "        n_components=2,\n",
    "        perplexity=perplexity,\n",
    "        init=init,\n",
    "        learning_rate=learning_rate,\n",
    "        max_iter=n_iter,\n",
    "        random_state=seed\n",
    "    )\n",
    "    Z = tsne.fit_transform(X_scaled)\n",
    "\n",
    "    sns.scatterplot(\n",
    "        x=Z[:, 0], y=Z[:, 1],\n",
    "        hue=y_str,\n",
    "        palette=\"deep\",\n",
    "        s=18, linewidth=0,\n",
    "        ax=ax, legend=False\n",
    "    )\n",
    "    ax.set_title(f\"t-SNE 2D | seed={seed}\")\n",
    "    ax.set_xlabel(\"\")\n",
    "    ax.set_ylabel(\"\")\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "\n",
    "plt.suptitle(f\"Instabilidade do t-SNE: mesma entrada, seeds diferentes (init='{init}', perplexity={perplexity})\", y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4600b644-ecf3-4ff4-bac7-883b673d1f0b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "## Comparação PCA vs t-SNE\n",
    "\n",
    "Ao comparar com o PCA:\n",
    "\n",
    "* **PCA** preserva estrutura global e variância máxima, produzindo projeções estáveis e determinísticas.\n",
    "* **t-SNE** prioriza vizinhança local, frequentemente produzindo separações visuais mais evidentes entre grupos.\n",
    "* PCA é adequado para pré-processamento e compressão.\n",
    "* t-SNE é indicado para visualização e análise qualitativa de embeddings.\n",
    "\n",
    "Essa comparação reforça que os métodos possuem objetivos distintos e devem ser utilizados de forma complementar no pipeline de análise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bb8787f9-7a35-47ed-bfab-12778c7917ae",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "\n",
    "labels = y.astype(str)\n",
    "\n",
    "# ----------------------------\n",
    "# PCA 2D/3D\n",
    "# ----------------------------\n",
    "pca2 = PCA(n_components=2, random_state=42)\n",
    "Z_pca2 = pca2.fit_transform(X_scaled)\n",
    "\n",
    "pca3 = PCA(n_components=3, random_state=42)\n",
    "Z_pca3 = pca3.fit_transform(X_scaled)\n",
    "\n",
    "# ----------------------------\n",
    "# t-SNE 2D/3D (visualização)\n",
    "# ----------------------------\n",
    "tsne2 = TSNE(\n",
    "    n_components=2,\n",
    "    perplexity=30,\n",
    "    init=\"pca\",\n",
    "    learning_rate=\"auto\",\n",
    "    max_iter=1000,\n",
    "    random_state=42\n",
    ")\n",
    "Z_tsne2 = tsne2.fit_transform(X_scaled)\n",
    "\n",
    "tsne3 = TSNE(\n",
    "    n_components=3,\n",
    "    perplexity=30,\n",
    "    init=\"pca\",\n",
    "    learning_rate=\"auto\",\n",
    "    max_iter=1000,\n",
    "    random_state=42\n",
    ")\n",
    "Z_tsne3 = tsne3.fit_transform(X_scaled)\n",
    "\n",
    "# ============================================================\n",
    "# PAINEL 2D: PCA vs t-SNE (lado a lado)\n",
    "# ============================================================\n",
    "df2_pca = pd.DataFrame({\"D1\": Z_pca2[:, 0], \"D2\": Z_pca2[:, 1], \"Class\": labels})\n",
    "df2_tsne = pd.DataFrame({\"D1\": Z_tsne2[:, 0], \"D2\": Z_tsne2[:, 1], \"Class\": labels})\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "sns.scatterplot(\n",
    "    data=df2_pca, x=\"D1\", y=\"D2\", hue=\"Class\",\n",
    "    palette=\"deep\", s=18, linewidth=0, ax=axes[0], legend=True\n",
    ")\n",
    "axes[0].set_title(f\"PCA 2D (var acumulada={pca2.explained_variance_ratio_.sum():.3f})\")\n",
    "axes[0].set_xticks([]); axes[0].set_yticks([])\n",
    "axes[0].set_xlabel(\"\"); axes[0].set_ylabel(\"\")\n",
    "\n",
    "sns.scatterplot(\n",
    "    data=df2_tsne, x=\"D1\", y=\"D2\", hue=\"Class\",\n",
    "    palette=\"deep\", s=18, linewidth=0, ax=axes[1], legend=False\n",
    ")\n",
    "axes[1].set_title(\"t-SNE 2D\")\n",
    "axes[1].set_xticks([]); axes[1].set_yticks([])\n",
    "axes[1].set_xlabel(\"\"); axes[1].set_ylabel(\"\")\n",
    "\n",
    "plt.suptitle(\"Comparativo 2D: PCA vs t-SNE\", y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# ============================================================\n",
    "# PAINEL 3D: PCA vs t-SNE (lado a lado, interativo Plotly)\n",
    "# ============================================================\n",
    "\n",
    "# Paleta Seaborn -> HEX\n",
    "classes = np.unique(labels)\n",
    "palette = sns.color_palette(\"deep\", len(classes))\n",
    "palette_hex = [mcolors.to_hex(c) for c in palette]\n",
    "color_map = {cls: palette_hex[i] for i, cls in enumerate(classes)}\n",
    "\n",
    "# Cria subplots 3D lado a lado\n",
    "from plotly.subplots import make_subplots\n",
    "fig3 = make_subplots(\n",
    "    rows=1, cols=2,\n",
    "    specs=[[{\"type\": \"scene\"}, {\"type\": \"scene\"}]],\n",
    "    subplot_titles=(\n",
    "        f\"PCA 3D (var acum={pca3.explained_variance_ratio_.sum():.3f})\",\n",
    "        \"t-SNE 3D\"\n",
    "    )\n",
    ")\n",
    "\n",
    "# PCA 3D traces\n",
    "for cls in classes:\n",
    "    idx = (labels == cls)\n",
    "    fig3.add_trace(\n",
    "        go.Scatter3d(\n",
    "            x=Z_pca3[idx, 0], y=Z_pca3[idx, 1], z=Z_pca3[idx, 2],\n",
    "            mode=\"markers\",\n",
    "            marker=dict(size=3, color=color_map[cls]),\n",
    "            name=f\"Class {cls}\",\n",
    "            showlegend=True\n",
    "        ),\n",
    "        row=1, col=1\n",
    "    )\n",
    "\n",
    "# t-SNE 3D traces\n",
    "for cls in classes:\n",
    "    idx = (labels == cls)\n",
    "    fig3.add_trace(\n",
    "        go.Scatter3d(\n",
    "            x=Z_tsne3[idx, 0], y=Z_tsne3[idx, 1], z=Z_tsne3[idx, 2],\n",
    "            mode=\"markers\",\n",
    "            marker=dict(size=3, color=color_map[cls]),\n",
    "            name=f\"Class {cls}\",\n",
    "            showlegend=False  # legenda já aparece no painel da esquerda\n",
    "        ),\n",
    "        row=1, col=2\n",
    "    )\n",
    "\n",
    "# Fundo branco + ajustes\n",
    "fig3.update_layout(\n",
    "    title=\"Comparativo 3D Interativo: PCA vs t-SNE\",\n",
    "    template=\"plotly_white\",\n",
    "    paper_bgcolor=\"white\",\n",
    "    plot_bgcolor=\"white\",\n",
    "    height=600,\n",
    "    legend=dict(itemsizing=\"constant\")\n",
    ")\n",
    "\n",
    "fig3.update_scenes(\n",
    "    bgcolor=\"white\",\n",
    "    xaxis=dict(showbackground=True, backgroundcolor=\"white\"),\n",
    "    yaxis=dict(showbackground=True, backgroundcolor=\"white\"),\n",
    "    zaxis=dict(showbackground=True, backgroundcolor=\"white\"),\n",
    ")\n",
    "\n",
    "fig3.show()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Redução de Dimensionalidade",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}