{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "yz6nCqCFLPsc",
        "jjqi9vCbVD6L"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/andrehochuli/teaching/blob/main/ComputerVision/Lecture%2005%20-%20Feature%20Extraction/Lecture_05_Avalia%C3%A7%C3%A3o_Formativa_Lecture_NOTES.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Auxiliary Functions"
      ],
      "metadata": {
        "id": "yz6nCqCFLPsc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import cv2, math\n",
        "import matplotlib.pyplot as plt\n",
        "#Keras to import datasets, not for deep learning (yet)\n",
        "from tensorflow import keras\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn import metrics, preprocessing\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.manifold import TSNE\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "import skimage.feature as feature\n",
        "from sklearn.model_selection import train_test_split\n",
        "import os\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.preprocessing import StandardScaler"
      ],
      "metadata": {
        "id": "8ffCHWAey-Xd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Auxiliary Function to plot side by side\n",
        "#@Author: Prof. André Hochuli\n",
        "#Visualiza um lista de figuras lado a lado, facilitando a comparação qualitativa\n",
        "def plot_sidebyside(img_list,titles=None,colormap=None,figsize=(12,6)):\n",
        "  n = len(img_list)\n",
        "  figure, axis = plt.subplots(1, n, figsize=figsize)\n",
        "\n",
        "  if titles is None:\n",
        "    titles = []\n",
        "    A = ord('A')\n",
        "    for i in range(n):\n",
        "      titles.append(chr(A+i))\n",
        "\n",
        "  for i in range(n):\n",
        "    axis[i].imshow(img_list[i], cmap=colormap)\n",
        "    axis[i].set_title(titles[i])\n",
        "    axis[i].axis('off')\n",
        "  # Combine all the operations and display\n",
        "  plt.show()"
      ],
      "metadata": {
        "id": "aXLNCO7kC8zj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@Author: Prof. André Hochuli\n",
        "#Compila os resultados para analises qualitativas e quantitativas\n",
        "def performance_evaluation(x_test, y_test, predictions, class_names, info_message):\n",
        "\n",
        "    print(f\"Evaluation of {info_message}\")\n",
        "    print(metrics.classification_report(y_test, predictions))\n",
        "\n",
        "\n",
        "    # Matriz de confusão\n",
        "    disp = metrics.ConfusionMatrixDisplay.from_predictions(y_test, predictions)\n",
        "    disp.figure_.suptitle(\"Confusion Matrix\")\n",
        "    plt.show()\n",
        "\n",
        "    # Imagens classificadas corretamente\n",
        "    correct_idx = np.where(y_test == predictions)[0]\n",
        "    n_correct = min(10, len(correct_idx))\n",
        "    if n_correct > 0:\n",
        "        plt.figure(figsize=(22, 4))\n",
        "        for i in range(n_correct):\n",
        "            idx = correct_idx[i]\n",
        "            plt.subplot(1, n_correct, i+1)\n",
        "            plt.imshow(x_test[idx], cmap='gray', interpolation='nearest')\n",
        "            plt.axis('off')\n",
        "            plt.title(f\"Lbl:{y_test[idx]} Pred:{predictions[idx]}\")\n",
        "        plt.suptitle(\"Correct Predictions\", fontsize=16, fontweight='bold', color='white', backgroundcolor='green')\n",
        "        plt.show()\n",
        "\n",
        "    #Imagens classificadas incorretamente\n",
        "    wrong_idx = np.where(y_test != predictions)[0]\n",
        "    n_wrong = min(10, len(wrong_idx))\n",
        "    if n_wrong > 0:\n",
        "        plt.figure(figsize=(22, 4))\n",
        "        for i in range(n_wrong):\n",
        "            idx = wrong_idx[i]\n",
        "            plt.subplot(1, n_wrong, i+1)\n",
        "            plt.imshow(x_test[idx], cmap='gray', interpolation='nearest')\n",
        "            plt.axis('off')\n",
        "            plt.title(f\"Lbl:{y_test[idx]} Pred:{predictions[idx]}\")\n",
        "        plt.suptitle(\"Wrong Predictions\", fontsize=16, fontweight='bold', color='white', backgroundcolor='red')\n",
        "        plt.show()\n",
        "\n",
        "\n",
        "    #Exibir exemplo de cada classe\n",
        "    unique_classes = np.unique(y_test)\n",
        "    plt.figure(figsize=(22, 4))\n",
        "    for i, cls in enumerate(unique_classes):\n",
        "        idx = np.where(y_test == cls)[0][0]  # primeiro índice da classe\n",
        "        plt.subplot(1, len(unique_classes), i+1)\n",
        "        plt.imshow(x_test[idx], cmap='gray', interpolation='nearest')\n",
        "        plt.axis('off')\n",
        "        plt.title(f\"{i}-{class_names[cls]}\")\n",
        "    plt.suptitle(\"Example of each class\", fontsize=16, fontweight='bold', color='black', backgroundcolor='yellow')\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "id": "k7qqZv31fMV-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def random_shuffle(X_train, y_train, X_test, y_test, random_state=42):\n",
        "    np.random.seed(random_state)\n",
        "\n",
        "    # Shuffle train\n",
        "    idx_train = np.random.permutation(len(y_train))\n",
        "    X_train_shuffled = X_train[idx_train]\n",
        "    y_train_shuffled = y_train[idx_train]\n",
        "\n",
        "    # Shuffle test\n",
        "    idx_test = np.random.permutation(len(y_test))\n",
        "    X_test_shuffled = X_test[idx_test]\n",
        "    y_test_shuffled = y_test[idx_test]\n",
        "\n",
        "    return X_train_shuffled, y_train_shuffled, X_test_shuffled, y_test_shuffled"
      ],
      "metadata": {
        "id": "7WxJQVr24HNS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from collections import Counter\n",
        "\n",
        "def random_undersampling(X, y, random_state=42):\n",
        "\n",
        "    np.random.seed(random_state)\n",
        "\n",
        "    # Contar amostras por classe\n",
        "    class_counts = Counter(y)\n",
        "    min_count = min(class_counts.values())\n",
        "\n",
        "    X_resampled = []\n",
        "    y_resampled = []\n",
        "\n",
        "    for cls in class_counts.keys():\n",
        "        idx_cls = np.where(y == cls)[0]\n",
        "        # Seleciona min_count índices aleatoriamente\n",
        "        selected_idx = np.random.choice(idx_cls, size=min_count, replace=False)\n",
        "        X_resampled.append(X[selected_idx])\n",
        "        y_resampled.append(y[selected_idx])\n",
        "\n",
        "    X_resampled = np.concatenate(X_resampled, axis=0)\n",
        "    y_resampled = np.concatenate(y_resampled, axis=0)\n",
        "\n",
        "    # Embaralhar os dados novamente\n",
        "    shuffle_idx = np.random.permutation(len(y_resampled))\n",
        "    X_resampled = X_resampled[shuffle_idx]\n",
        "    y_resampled = y_resampled[shuffle_idx]\n",
        "\n",
        "    return X_resampled, y_resampled\n"
      ],
      "metadata": {
        "id": "sqWuwZWMwkF0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def retrieve_swedish_leaf(base_dir):\n",
        "    dataset_url = \"http://www.ppgia.pucpr.br/~aghochuli/swedish_leaf.zip\"\n",
        "    zip_path = \"swedish_leaf.zip\"\n",
        "\n",
        "    if not os.path.exists(base_dir):\n",
        "      print(\"Downloading...\")\n",
        "      os.system(f\"wget -O {zip_path} {dataset_url}\")\n",
        "      os.system(f\"unzip {zip_path}\")\n",
        "      #os.remove(zip_path)\n",
        "      print(f\"Dataset extracted to: {base_dir}\")\n",
        "    else:\n",
        "      print(f\"Dataset already available at: {base_dir}\")\n",
        "\n",
        "def load_swedish_leaf():\n",
        "\n",
        "    base_dir = \"swedish_leaf\"\n",
        "\n",
        "    retrieve_swedish_leaf(base_dir)\n",
        "\n",
        "    x_train = []\n",
        "    y_train = []\n",
        "\n",
        "    for i in range(1, 9):\n",
        "        leaf_dir = os.path.join(base_dir, f'leaf{i}')\n",
        "        if os.path.isdir(leaf_dir):\n",
        "\n",
        "            for filename in os.listdir(leaf_dir):\n",
        "                if filename.endswith('.png'):\n",
        "                    img_path = os.path.join(leaf_dir, filename)\n",
        "                    try:\n",
        "                        # Carrega em escala de cinza\n",
        "                        img = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)\n",
        "                        # Redimensiona para 128x128\n",
        "                        img_resized = cv2.resize(img, (128, 128))\n",
        "\n",
        "                        x_train.append(img_resized)\n",
        "                        y_train.append(i - 1)  # Labels de 0 a 7\n",
        "                    except Exception as e:\n",
        "                        print(f\"Error loading image {img_path}: {e}\")\n",
        "\n",
        "    # Holdout 70/30 estratificado\n",
        "    X_train, X_test, y_train_split, y_test_split = train_test_split(\n",
        "        x_train, y_train,\n",
        "        test_size=0.3,\n",
        "        random_state=42,\n",
        "        stratify=y_train\n",
        "    )\n",
        "\n",
        "    class_names = {\n",
        "      0: 'Ulmus carpinifolia',\n",
        "      1: 'Acer',\n",
        "      2: 'Salix aurita',\n",
        "      3: 'Quercus',\n",
        "      4: 'Alnus incana',\n",
        "      5: 'Betula pubescens',\n",
        "      6: 'Salix alba \\'Sericea\\'',\n",
        "      7: 'Populus tremula'\n",
        "    }\n",
        "\n",
        "    return np.array(X_train),np.array(y_train_split),np.array(X_test),np.array(y_test_split), class_names"
      ],
      "metadata": {
        "id": "DGVNNhXEd7Fu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def retrieve_paper_rock_scissors(base_dir):\n",
        "    dataset_url = \"http://www.ppgia.pucpr.br/~aghochuli/paper-rock-scissors.zip\"\n",
        "    zip_path = \"paper-rock-scissors.zip\"\n",
        "\n",
        "    if not os.path.exists(base_dir):\n",
        "        print(\"Downloading...\")\n",
        "        os.system(f\"wget -O {zip_path} {dataset_url}\")\n",
        "        os.system(f\"unzip {zip_path}\")\n",
        "        #os.remove(zip_path)\n",
        "        print(f\"Dataset extracted to: {base_dir}\")\n",
        "    else:\n",
        "        print(f\"Dataset already available at: {base_dir}\")\n",
        "\n",
        "def load_paper_rock_scissors():\n",
        "\n",
        "    base_dir = \"paper-rock-scissors\"\n",
        "\n",
        "    retrieve_paper_rock_scissors(base_dir)\n",
        "\n",
        "    x_data = []\n",
        "    y_data = []\n",
        "\n",
        "    # Mapeamento das classes\n",
        "    class_names = {\n",
        "        0: 'paper',\n",
        "        1: 'rock',\n",
        "        2: 'scissors'\n",
        "    }\n",
        "\n",
        "    for label, cls in class_names.items():\n",
        "        cls_dir = os.path.join(base_dir, cls)\n",
        "        if os.path.isdir(cls_dir):\n",
        "            for filename in os.listdir(cls_dir):\n",
        "                if filename.endswith(('.png', '.jpg', '.jpeg')):\n",
        "                    img_path = os.path.join(cls_dir, filename)\n",
        "                    try:\n",
        "                        # Carrega em escala de cinza\n",
        "                        img = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)\n",
        "                        # Redimensiona para 128x128\n",
        "                        img_resized = cv2.resize(img, (128, 128))\n",
        "                        x_data.append(img_resized)\n",
        "                        y_data.append(label)\n",
        "                    except Exception as e:\n",
        "                        print(f\"Error loading image {img_path}: {e}\")\n",
        "\n",
        "    # Holdout 70/30 estratificado\n",
        "    X_train, X_test, y_train_split, y_test_split = train_test_split(\n",
        "        x_data, y_data,\n",
        "        test_size=0.3,\n",
        "        random_state=42,\n",
        "        stratify=y_data\n",
        "    )\n",
        "\n",
        "    return (\n",
        "        np.array(X_train),\n",
        "        np.array(y_train_split),\n",
        "        np.array(X_test),\n",
        "        np.array(y_test_split),\n",
        "        class_names\n",
        "    )"
      ],
      "metadata": {
        "id": "B5BP0kjHtuks"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "\n",
        "def retrieve_vehicle(base_dir):\n",
        "    dataset_url = \"http://www.ppgia.pucpr.br/~aghochuli/vehicle.zip\"\n",
        "    zip_path = \"vehicle.zip\"\n",
        "\n",
        "    if not os.path.exists(base_dir):\n",
        "        print(\"Downloading...\")\n",
        "        os.system(f\"wget -O {zip_path} {dataset_url}\")\n",
        "        os.system(f\"unzip {zip_path}\")\n",
        "        #os.remove(zip_path)\n",
        "        print(f\"Dataset extracted to: {base_dir}\")\n",
        "    else:\n",
        "        print(f\"Dataset already available at: {base_dir}\")\n",
        "\n",
        "def load_vehicle():\n",
        "    base_dir = \"vehicle\"\n",
        "    retrieve_vehicle(base_dir)\n",
        "\n",
        "    # Estrutura esperada\n",
        "    subsets = ['train', 'test']\n",
        "    class_names = {}\n",
        "    data = {}\n",
        "\n",
        "    for subset in subsets:\n",
        "        subset_dir = os.path.join(base_dir, subset)\n",
        "        x_data = []\n",
        "        y_data = []\n",
        "        if os.path.isdir(subset_dir):\n",
        "            classes = [d for d in os.listdir(subset_dir) if os.path.isdir(os.path.join(subset_dir, d))]\n",
        "            # Mapear classes para labels se ainda não mapeadas\n",
        "            for idx, cls in enumerate(classes):\n",
        "                if cls not in class_names.values():\n",
        "                    class_names[idx] = cls\n",
        "            # Carregar imagens\n",
        "            for label, cls in class_names.items():\n",
        "                cls_dir = os.path.join(subset_dir, cls)\n",
        "                if os.path.isdir(cls_dir):\n",
        "                    for filename in os.listdir(cls_dir):\n",
        "                        if filename.endswith(('.png', '.jpg', '.jpeg')):\n",
        "                            img_path = os.path.join(cls_dir, filename)\n",
        "                            try:\n",
        "                                img = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)\n",
        "                                img_resized = cv2.resize(img, (128, 128))\n",
        "                                x_data.append(img_resized)\n",
        "                                y_data.append(label)\n",
        "                            except Exception as e:\n",
        "                                print(f\"Error loading image {img_path}: {e}\")\n",
        "        data[subset] = (np.array(x_data), np.array(y_data))\n",
        "\n",
        "    return data['train'][0], data['train'][1], data['test'][0], data['test'][1], class_names"
      ],
      "metadata": {
        "id": "LJ3UCnn6v0C0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Descriptors"
      ],
      "metadata": {
        "id": "jjqi9vCbVD6L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class EdgeDescriptor:\n",
        "    def __init__(self, method='canny', low_threshold=50, high_threshold=150):\n",
        "        \"\"\"\n",
        "        Descritor baseado em bordas.\n",
        "        method: 'canny' ou 'sobel'\n",
        "        \"\"\"\n",
        "        self.method = method.lower()\n",
        "        self.low_threshold = low_threshold\n",
        "        self.high_threshold = high_threshold\n",
        "\n",
        "    def describe(self, image, visualize=False):\n",
        "        \"\"\"\n",
        "        Retorna um vetor de features baseado em bordas.\n",
        "        Se visualize=True, retorna também a imagem de bordas.\n",
        "        \"\"\"\n",
        "        # Converte para escala de cinza se necessário\n",
        "        if len(image.shape) > 2 and image.shape[2] == 3:\n",
        "            gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
        "        else:\n",
        "            gray = image.copy()\n",
        "\n",
        "        # Aplicar detector de bordas\n",
        "        if self.method == 'canny':\n",
        "            edges = cv2.Canny(gray, self.low_threshold, self.high_threshold)\n",
        "        elif self.method == 'sobel':\n",
        "            sobelx = cv2.Sobel(gray, cv2.CV_64F, 1, 0, ksize=3)\n",
        "            sobely = cv2.Sobel(gray, cv2.CV_64F, 0, 1, ksize=3)\n",
        "            edges = cv2.magnitude(sobelx, sobely)\n",
        "            edges = np.uint8(edges)\n",
        "        else:\n",
        "            raise ValueError(\"method deve ser 'canny' ou 'sobel'\")\n",
        "\n",
        "        if visualize:\n",
        "            return edges.flatten(), edges\n",
        "        else:\n",
        "            return edges.flatten()\n"
      ],
      "metadata": {
        "id": "hDfFQ8FYKqFn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ProjectionHistogramDescriptor:\n",
        "    def __init__(self, normalize=True):\n",
        "        \"\"\"\n",
        "        Descritor baseado em histogramas de projeção horizontal e vertical.\n",
        "        normalize: se True, normaliza os histogramas\n",
        "        \"\"\"\n",
        "        self.normalize = normalize\n",
        "\n",
        "    def describe(self, image, visualize=False):\n",
        "        \"\"\"\n",
        "        Retorna o vetor de features de projeção.\n",
        "        Se visualize=True, retorna também uma figura com histogramas.\n",
        "        \"\"\"\n",
        "        # Converte para escala de cinza se necessário\n",
        "        if len(image.shape) > 2 and image.shape[2] == 3:\n",
        "            gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
        "        else:\n",
        "            gray = image.copy()\n",
        "\n",
        "        # Histogramas de projeção\n",
        "        horizontal_proj = np.sum(gray, axis=1)\n",
        "        vertical_proj = np.sum(gray, axis=0)\n",
        "\n",
        "        if self.normalize:\n",
        "            horizontal_proj = horizontal_proj / (horizontal_proj.sum() + 1e-7)\n",
        "            vertical_proj = vertical_proj / (vertical_proj.sum() + 1e-7)\n",
        "\n",
        "        features = np.concatenate([horizontal_proj, vertical_proj])\n",
        "\n",
        "        if visualize:\n",
        "            fig, ax = plt.subplots(1, 2, figsize=(10,3))\n",
        "            ax[0].bar(range(len(horizontal_proj)), horizontal_proj, color='gray')\n",
        "            ax[0].set_title(\"Horizontal Projection\")\n",
        "            ax[1].bar(range(len(vertical_proj)), vertical_proj, color='gray')\n",
        "            ax[1].set_title(\"Vertical Projection\")\n",
        "            plt.tight_layout()\n",
        "            plt.close(fig)  # não exibe automaticamente\n",
        "            return features, fig\n",
        "        else:\n",
        "            return features"
      ],
      "metadata": {
        "id": "HlRxNWvhKueu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ColorDescriptor:\n",
        "    def __init__(self, bins=(8, 8, 8), color_space='HSV'):\n",
        "        \"\"\"\n",
        "        bins: número de bins por canal\n",
        "        color_space: 'HSV' ou 'RGB'\n",
        "        \"\"\"\n",
        "        self.bins = bins\n",
        "        self.color_space = color_space.upper()\n",
        "        if self.color_space not in ['HSV', 'RGB']:\n",
        "            raise ValueError(\"color_space deve ser 'HSV' ou 'RGB'\")\n",
        "\n",
        "    def describe(self, image, mask=None, visualize=False):\n",
        "        \"\"\"\n",
        "        Retorna o histograma de cor concatenado e normalizado.\n",
        "        Se visualize=True, retorna também a imagem do histograma.\n",
        "        \"\"\"\n",
        "        # Converte a imagem para o espaço de cor desejado\n",
        "        if self.color_space == 'HSV':\n",
        "            image_conv = cv2.cvtColor(image, cv2.COLOR_BGR2HSV) if image.shape[2]==3 else image\n",
        "            hist_range = [0, 180, 0, 256, 0, 256]\n",
        "        else:  # RGB\n",
        "            image_conv = cv2.cvtColor(image, cv2.COLOR_BGR2RGB) if image.shape[2]==3 else image\n",
        "            hist_range = [0, 256, 0, 256, 0, 256]\n",
        "\n",
        "        # Calcula histograma 3D\n",
        "        hist = cv2.calcHist([image_conv], [0, 1, 2], mask, self.bins, hist_range)\n",
        "        hist = cv2.normalize(hist, hist).flatten()\n",
        "\n",
        "        if visualize:\n",
        "            # Criar uma imagem simples do histograma para inspeção\n",
        "            fig, ax = plt.subplots(figsize=(5,2))\n",
        "            ax.bar(range(len(hist)), hist, color='gray')\n",
        "            ax.set_title(f\"{self.color_space} Histogram\")\n",
        "            ax.set_xlabel(\"Bins\")\n",
        "            ax.set_ylabel(\"Normalized Count\")\n",
        "            ax.set_xticks([])\n",
        "            plt.close(fig)  # não exibe automaticamente\n",
        "            return hist, fig\n",
        "        else:\n",
        "            return hist"
      ],
      "metadata": {
        "id": "OTx92R5SKI6Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class HuMomentsDescriptor:\n",
        "    def describe(self, im, threshold_val=128, visualize=False):\n",
        "        \"\"\"\n",
        "        Calcula o vetor de Hu Moments de uma imagem.\n",
        "        - im: imagem RGB ou grayscale\n",
        "        - threshold_val: valor para binarização\n",
        "        - visualize: se True, retorna também a imagem binarizada\n",
        "        \"\"\"\n",
        "        # Converte para grayscale se necessário\n",
        "        if len(im.shape) > 2 and im.shape[2] == 3:\n",
        "            im = cv2.cvtColor(im, cv2.COLOR_RGB2GRAY)\n",
        "\n",
        "        # Binariza a imagem\n",
        "        _, thresh = cv2.threshold(im, threshold_val, 255, cv2.THRESH_BINARY)\n",
        "\n",
        "        # Calcula momentos e Hu Moments\n",
        "        moments = cv2.moments(thresh)\n",
        "        huMoments = cv2.HuMoments(moments)\n",
        "\n",
        "        # Escala logarítmica\n",
        "        for i in range(len(huMoments)):\n",
        "            if huMoments[i] != 0:\n",
        "                huMoments[i] = -1 * math.copysign(1.0, huMoments[i]) * math.log10(abs(huMoments[i]))\n",
        "\n",
        "        hu_vector = huMoments.reshape(-1)\n",
        "\n",
        "        if visualize:\n",
        "            return hu_vector, thresh\n",
        "        else:\n",
        "            return hu_vector"
      ],
      "metadata": {
        "id": "oJ8V-yg39uwU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class HogDescriptor:\n",
        "    def __init__(self, orientations=9, pixels_per_cell=(8,8), cells_per_block=(2,2), resize_dim=(64,128)):\n",
        "        \"\"\"\n",
        "        Classe para extrair features HOG.\n",
        "        \"\"\"\n",
        "        self.orientations = orientations\n",
        "        self.pixels_per_cell = pixels_per_cell\n",
        "        self.cells_per_block = cells_per_block\n",
        "        self.resize_dim = resize_dim\n",
        "\n",
        "    def describe(self, im, visualize=False):\n",
        "        \"\"\"\n",
        "        Extrai o vetor de features HOG de uma imagem.\n",
        "        - im: imagem RGB ou grayscale\n",
        "        - visualize: se True, retorna também a imagem HOG\n",
        "        \"\"\"\n",
        "        # Converte para grayscale se necessário\n",
        "        if len(im.shape) > 2 and im.shape[2] == 3:\n",
        "            im = cv2.cvtColor(im, cv2.COLOR_RGB2GRAY)\n",
        "\n",
        "        # Redimensiona a imagem\n",
        "        im = cv2.resize(im, self.resize_dim)\n",
        "\n",
        "        # Extrai HOG\n",
        "        fd, hog_image = feature.hog(im,\n",
        "                                    orientations=self.orientations,\n",
        "                                    pixels_per_cell=self.pixels_per_cell,\n",
        "                                    cells_per_block=self.cells_per_block,\n",
        "                                    block_norm='L2-Hys',\n",
        "                                    visualize=True)\n",
        "        if visualize:\n",
        "            return fd, hog_image\n",
        "        else:\n",
        "            return fd"
      ],
      "metadata": {
        "id": "Bpg5lTET9vJo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class GaborDescriptor:\n",
        "    def __init__(self, ksize=31, n_filters=8, sigma=4.0, lambd=10.0, gamma=0.5, psi=0):\n",
        "        \"\"\"\n",
        "        Classe para extrair features Gabor compactas.\n",
        "        Cada filtro gera estatísticas (média, variância, desvio padrão) para reduzir dimensionalidade.\n",
        "        \"\"\"\n",
        "        self.filters = []\n",
        "        for theta in np.arange(0, np.pi, np.pi / n_filters):\n",
        "            kern = cv2.getGaborKernel((ksize, ksize), sigma, theta, lambd, gamma, psi, ktype=cv2.CV_32F)\n",
        "            kern /= (kern.sum() + 1e-7)  # normalização segura\n",
        "            self.filters.append(kern)\n",
        "\n",
        "    def get_filters(self):\n",
        "        \"\"\"Retorna a lista de filtros\"\"\"\n",
        "        return self.filters\n",
        "\n",
        "    def describe(self, im, visualize=False):\n",
        "        \"\"\"\n",
        "        Retorna um vetor compacto de features Gabor baseado em estatísticas.\n",
        "        Se visualize=True, retorna também a imagem filtrada do primeiro filtro.\n",
        "        - im: imagem RGB ou escala de cinza\n",
        "        \"\"\"\n",
        "        # Converte para escala de cinza se necessário\n",
        "        if len(im.shape) > 2 and im.shape[2] == 3:\n",
        "            im_gray = cv2.cvtColor(im, cv2.COLOR_RGB2GRAY)\n",
        "        else:\n",
        "            im_gray = im.copy()\n",
        "\n",
        "        im_gray = cv2.resize(im_gray, (128, 128)).astype(np.float32)\n",
        "\n",
        "        feats = []\n",
        "        first_filtered = None\n",
        "        for i, kern in enumerate(self.filters):\n",
        "            f_im = cv2.filter2D(im_gray, cv2.CV_32F, kern)\n",
        "            feats.extend([f_im.mean(), f_im.var(), f_im.std()])\n",
        "            if visualize and i == 0:\n",
        "                first_filtered = f_im.copy()\n",
        "\n",
        "        if visualize:\n",
        "            return np.array(feats), first_filtered\n",
        "        else:\n",
        "            return np.array(feats)\n"
      ],
      "metadata": {
        "id": "SgZRep6l-Gel"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class LocalBinaryPatternsDescriptor:\n",
        "    def __init__(self, numPoints, radius):\n",
        "        \"\"\"\n",
        "        Descritor LBP (Local Binary Patterns).\n",
        "        numPoints: número de vizinhos\n",
        "        radius: raio\n",
        "        \"\"\"\n",
        "        self.numPoints = numPoints\n",
        "        self.radius = radius\n",
        "\n",
        "    def describe(self, image, eps=1e-7, visualize=False):\n",
        "        \"\"\"\n",
        "        Retorna o histograma normalizado LBP.\n",
        "        Se visualize=True, retorna também a imagem LBP.\n",
        "        \"\"\"\n",
        "        # Converte para grayscale se necessário\n",
        "        if len(image.shape) > 2 and image.shape[2] == 3:\n",
        "            gray = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)\n",
        "        else:\n",
        "            gray = image.copy()\n",
        "\n",
        "        # Calcula LBP\n",
        "        lbp_img = feature.local_binary_pattern(gray, self.numPoints, self.radius, method=\"uniform\")\n",
        "        hist, _ = np.histogram(lbp_img.ravel(),\n",
        "                               bins=np.arange(0, self.numPoints + 3),\n",
        "                               range=(0, self.numPoints + 2))\n",
        "        # Normaliza\n",
        "        hist = hist.astype('float')\n",
        "        hist /= (hist.sum() + eps)\n",
        "\n",
        "        if visualize:\n",
        "            return hist, lbp_img\n",
        "        else:\n",
        "            return hist"
      ],
      "metadata": {
        "id": "8N4FSuuB9scF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load Data"
      ],
      "metadata": {
        "id": "kCdD2lqtgVPi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#x_train, y_train, x_test, y_test, class_names = load_swedish_leaf()\n",
        "#x_train, y_train, x_test, y_test, class_names = load_paper_rock_scissors()\n",
        "x_train, y_train, x_test, y_test, class_names = load_vehicle()\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "786AbjISnwAb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Embaralha para evitar viés de classe\n",
        "x_train, y_train, x_test, y_test = random_shuffle(x_train, y_train, x_test, y_test)"
      ],
      "metadata": {
        "id": "7Oz-2Av-7LAo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_train, y_train = random_undersampling(x_train,y_train)"
      ],
      "metadata": {
        "id": "p1jPA5Be5ce_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"x_train shape:\", x_train.shape)\n",
        "print(\"y_train shape:\", y_train.shape)\n",
        "print(\"x_test shape:\", x_test.shape)\n",
        "print(\"y_test shape:\", y_test.shape)\n",
        "\n",
        "#y_all = np.concatenate([y_train, y_test])\n",
        "y_all = y_train\n",
        "unique_classes, counts = np.unique(y_all, return_counts=True)\n",
        "\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.bar(range(len(unique_classes)), counts, tick_label=[class_names[i] for i in unique_classes])\n",
        "plt.xticks(rotation=90)\n",
        "plt.xlabel(\"Classes\")\n",
        "plt.ylabel(\"Número de amostras\")\n",
        "plt.title(\"Distribuição das classes\")\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "XjJMLLospCYF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "samples_per_class = 5\n",
        "for cls in np.unique(y_train):\n",
        "    idxs = np.where(y_train == cls)[0]\n",
        "    idxs = np.random.choice(idxs, samples_per_class, replace=False)\n",
        "    imgs = [x_train[i] for i in idxs]\n",
        "    titles = [f\"{class_names[cls]} #{i+1}\" for i in range(samples_per_class)]\n",
        "    plot_sidebyside(imgs, titles=titles, colormap=\"gray\")"
      ],
      "metadata": {
        "id": "L3FUMyj9eqyu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "features_train = {}\n",
        "features_test = {}\n",
        "\n",
        "descriptors = [\n",
        "    LocalBinaryPatternsDescriptor(8, 2),\n",
        "    HogDescriptor(),\n",
        "    Projecto(),\n",
        "    GaborDescriptor(n_filters=8)\n",
        "]\n",
        "\n",
        "desc_names = ['LBP', 'HOG', 'HU', 'GABOR']\n",
        "\n",
        "for desc, name in zip(descriptors, desc_names):\n",
        "    print(f\"Extraindo features {name}...\")\n",
        "    feat_tr = np.array([desc.describe(img) for img in x_train]).reshape(len(x_train), -1)\n",
        "    feat_te = np.array([desc.describe(img) for img in x_test]).reshape(len(x_test), -1)\n",
        "    features_train[name] = feat_tr\n",
        "    features_test[name] = feat_te\n",
        "    print(f\"{name} -> train: {feat_tr.shape}, test: {feat_te.shape}\")\n"
      ],
      "metadata": {
        "id": "pN-eli3YP6fX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "classifiers = {\n",
        "    'SVM': SVC(kernel='linear', C=1.0),\n",
        "    'RandomForest': RandomForestClassifier(n_estimators=100),\n",
        "    'MLP': MLPClassifier(hidden_layer_sizes=(128,64), max_iter=300)\n",
        "}\n",
        "\n",
        "# Testar cada classificador\n",
        "for clf_name, clf in classifiers.items():\n",
        "    print(f\"\\n=== Classificador: {clf_name} ===\")\n",
        "    for desc_name in desc_names:\n",
        "        print('#' * 35)\n",
        "        print(f\"###### Usando features: {desc_name} #####\")\n",
        "        print('#' * 35)\n",
        "\n",
        "        X_tr = features_train[desc_name]\n",
        "        X_te = features_test[desc_name]\n",
        "\n",
        "        # Aplicar StandardScaler\n",
        "        scaler = StandardScaler()\n",
        "        X_tr_scaled = scaler.fit_transform(X_tr)\n",
        "        X_te_scaled = scaler.transform(X_te)\n",
        "\n",
        "\n",
        "        # Treinar\n",
        "        clf.fit(X_tr_scaled, y_train)\n",
        "        preds = clf.predict(X_te_scaled)\n",
        "\n",
        "        # Avaliar usando a função performance_evaluation\n",
        "        performance_evaluation(x_test, y_test, preds, class_names, f\"{clf_name} com {desc_name}\")\n",
        "\n"
      ],
      "metadata": {
        "id": "D9cUZe4QQjKE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Combinar todos os descritores\n",
        "X_train_combined = np.concatenate(list(features_train.values()), axis=1)\n",
        "X_test_combined = np.concatenate(list(features_test.values()), axis=1)\n",
        "\n",
        "print(\"Combinado -> train:\", X_train_combined.shape, \"test:\", X_test_combined.shape)\n",
        "\n",
        "\n",
        "# Normalização\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train_combined)\n",
        "X_test_scaled = scaler.transform(X_test_combined)\n",
        "\n",
        "\n",
        "# Reduzir dimensionalidade com PCA\n",
        "pca = PCA(n_components=0.97, random_state=42)  # mantém 97% da variância\n",
        "X_train_pca = pca.fit_transform(X_train_scaled)\n",
        "X_test_pca = pca.transform(X_test_scaled)\n",
        "\n",
        "print(\"Após PCA -> train:\", X_train_pca.shape, \"test:\", X_test_pca.shape)\n",
        "\n",
        "# Definição dos classificadores\n",
        "classifiers = {\n",
        "    'SVM': SVC(kernel='linear', C=1.0),\n",
        "    'RandomForest': RandomForestClassifier(n_estimators=100),\n",
        "    'MLP': MLPClassifier(hidden_layer_sizes=(128,64), max_iter=300)\n",
        "}\n",
        "\n",
        "# Treinamento e avaliação\n",
        "for name, clf in classifiers.items():\n",
        "    print(f\"\\nTreinando {name}...\")\n",
        "    clf.fit(X_train_pca, y_train)\n",
        "    acc = clf.score(X_test_pca, y_test)\n",
        "    print(f\"{name} Accuracy: {acc:.4f}\")"
      ],
      "metadata": {
        "id": "KoF6NDe10-oF"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}