{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP34dBsZQ4uhGQbSVEGmvxy",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/andrehochuli/teaching/blob/main/ComputerVision/Lecture%2010%20-CNN%20Applications%20and%20Tricks/Exercicio_Simpsons.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#The Simpsons classification"
      ],
      "metadata": {
        "id": "784J3sz4b6ss"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import cv2, glob\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds\n",
        "from tensorflow.keras import datasets, layers, models, losses\n",
        "from tensorflow.keras.models import Model\n",
        "from sklearn import metrics\n",
        "\n",
        "#To ensure reproducibility\n",
        "#we set the random seed\n",
        "seed_number = 10\n",
        "tf.random.set_seed(seed_number)\n",
        "np.random.seed(seed_number)"
      ],
      "metadata": {
        "id": "3aktU634cUyS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Auxiliary Function to plot images side by side\n",
        "def plot_sidebyside(img_list,titles,colormap=None,figsize=(12,6)):\n",
        "  n = len(img_list)\n",
        "  figure, axis = plt.subplots(1, n, figsize=figsize)\n",
        "\n",
        "  for i in range(n):\n",
        "    axis[i].imshow(img_list[i], cmap=colormap)\n",
        "    axis[i].set_title(titles[i])\n",
        "    axis[i].axis('off')\n",
        "  # Combine all the operations and display\n",
        "  plt.show()\n",
        "\n",
        "#Plot dataset samples\n",
        "def plot_dataset(ds, lbls_name):\n",
        "  N_SAMPLES = 10\n",
        "  for i in range(5):\n",
        "    for x,y in ds.take(1):\n",
        "\n",
        "      x = x.numpy()\n",
        "      x = np.squeeze(x)\n",
        "      y = y.numpy()\n",
        "      plot_sidebsyide(x[:N_SAMPLES],\n",
        "                      y[:N_SAMPLES],'gray')\n",
        "\n",
        "#Plot a training history\n",
        "def plot_history(history):\n",
        "  print(history.history.keys())\n",
        "  # summarize history for accuracy\n",
        "  plt.plot(history.history['acc'])\n",
        "  plt.plot(history.history['val_acc'])\n",
        "  plt.title('model accuracy')\n",
        "  plt.ylabel('accuracy')\n",
        "  plt.xlabel('epoch')\n",
        "  plt.legend(['train', 'val'], loc='upper left')\n",
        "  plt.show()\n",
        "  # summarize history for loss\n",
        "  plt.plot(history.history['loss'])\n",
        "  plt.plot(history.history['val_loss'])\n",
        "  plt.title('model loss')\n",
        "  plt.ylabel('loss')\n",
        "  plt.xlabel('epoch')\n",
        "  plt.legend(['train', 'val'], loc='upper left')\n",
        "  plt.show()"
      ],
      "metadata": {
        "id": "ASk5xIAld07X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://github.com/andrehochuli/teaching/raw/main/datasets/Simpsons.zip -O Simpsons.zip"
      ],
      "metadata": {
        "id": "73h5fivUcADS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip -qo Simpsons.zip"
      ],
      "metadata": {
        "id": "Xxbuki0qcfaS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_dir = './Simpsons/Treino'\n",
        "batch_size_ = 32\n",
        "input_shape_ = (96,96,3)\n",
        "\n",
        "train_ds = tf.keras.utils.image_dataset_from_directory(\n",
        "  data_dir,\n",
        "  validation_split=0.2,\n",
        "  subset=\"training\",\n",
        "  seed=seed_number,\n",
        "  image_size=(input_shape_[0], input_shape_[1]),\n",
        "  batch_size=batch_size_)\n",
        "\n",
        "val_ds = tf.keras.utils.image_dataset_from_directory(\n",
        "  data_dir,\n",
        "  validation_split=0.2,\n",
        "  subset=\"validation\",\n",
        "  seed=seed_number,\n",
        "  image_size=(input_shape_[0], input_shape_[1]),\n",
        "  batch_size=batch_size_)"
      ],
      "metadata": {
        "id": "ffsGOJk-dK9I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class_names = train_ds.class_names\n",
        "num_classes = len(class_names)\n",
        "print(class_names)\n",
        "class_names = np.array(class_names)\n",
        "for img,lbl in train_ds.take(5):\n",
        "\n",
        "  titles = []\n",
        "  print(lbl)\n",
        "  for i in lbl:\n",
        "    titles.append(class_names[i])\n",
        "\n",
        "  plot_sidebyside(img[:5]/255.,titles[:5])"
      ],
      "metadata": {
        "id": "ogWgc6jDeATw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_train = []\n",
        "x_train = []\n",
        "#test_ds = test_ds.map(lambda x, y: (layers.Rescaling(1./255)(x), y))\n",
        "\n",
        "for img, labels in train_ds:\n",
        "    y_train.extend(labels.numpy())\n",
        "    x_train.extend(img)\n",
        "\n",
        "y_train = np.array(y_train)\n",
        "x_train = np.array(x_train)"
      ],
      "metadata": {
        "id": "iZVc-rg7fLlj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ret = np.unique(y_train,return_counts=True)\n",
        "print(class_names)\n",
        "print(ret)"
      ],
      "metadata": {
        "id": "yVVNJ3QMfdhl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = models.Sequential()\n",
        "\n",
        "#Feature Learning (Convolutions)\n",
        "model.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=input_shape_))\n",
        "model.add(layers.MaxPooling2D((2, 2)))\n",
        "\n",
        "model.add(layers.Conv2D(64, (3, 3), activation='relu', input_shape=input_shape_))\n",
        "model.add(layers.MaxPooling2D((2, 2)))\n",
        "\n",
        "model.add(layers.Conv2D(128, (3, 3), activation='relu', input_shape=input_shape_))\n",
        "model.add(layers.MaxPooling2D((2, 2)))\n",
        "\n",
        "model.add(layers.Conv2D(64, (3, 3), activation='relu', input_shape=input_shape_))\n",
        "model.add(layers.MaxPooling2D((2, 2)))\n",
        "\n",
        "model.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=input_shape_))\n",
        "model.add(layers.MaxPooling2D((2, 2)))\n",
        "######################################\n",
        "\n",
        "# Fully-Connect (Classifcation)\n",
        "model.add(layers.Dropout(0.25))\n",
        "model.add(layers.Flatten())\n",
        "model.add(layers.Dense(512, activation='relu'))\n",
        "model.add(layers.Dense(128, activation='relu'))\n",
        "model.add(layers.Dense(num_classes,activation='softmax'))\n",
        "\n",
        "\n",
        "\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "QFXGj5YVgCFn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "epochs_ = 10\n",
        "model.compile(optimizer=\"Adam\", loss=\"sparse_categorical_crossentropy\", metrics=[\"acc\"])\n",
        "history = model.fit(train_ds, batch_size=batch_size_, epochs=epochs_, validation_data=val_ds)"
      ],
      "metadata": {
        "id": "-tMaQW58hHmS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_history(history)"
      ],
      "metadata": {
        "id": "uWFCY25ThbGl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = models.Sequential()\n",
        "\n",
        "#Feature Learning (Convolutions)\n",
        "model.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=input_shape_))\n",
        "model.add(layers.MaxPooling2D((2, 2)))\n",
        "\n",
        "model.add(layers.Conv2D(64, (3, 3), activation='relu', input_shape=input_shape_))\n",
        "model.add(layers.Conv2D(128, (3, 3), activation='relu', input_shape=input_shape_))\n",
        "model.add(layers.Conv2D(64, (3, 3), activation='relu', input_shape=input_shape_))\n",
        "model.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=input_shape_))\n",
        "\n",
        "######################################\n",
        "\n",
        "# Fully-Connect (Classifcation)\n",
        "model.add(layers.Dropout(0.10))\n",
        "model.add(layers.Flatten())\n",
        "model.add(layers.Dense(512, activation='relu'))\n",
        "model.add(layers.Dense(128, activation='relu'))\n",
        "model.add(layers.Dense(num_classes,activation='softmax'))\n",
        "\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "ieGpWMp-hzsx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "epochs_ = 10\n",
        "model.compile(optimizer=\"Adam\", loss=\"sparse_categorical_crossentropy\", metrics=[\"acc\"])\n",
        "history = model.fit(train_ds, batch_size=batch_size_, epochs=epochs_, validation_data=val_ds)"
      ],
      "metadata": {
        "id": "pEgxlHmYi7jh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Data Augmentation"
      ],
      "metadata": {
        "id": "CLDe8VhplrSI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data_augmentation = tf.keras.Sequential(\n",
        "  [\n",
        "    layers.RandomFlip(\"horizontal\",\n",
        "                      input_shape=(input_shape_[0],\n",
        "                                  input_shape_[1],\n",
        "                                  3)),\n",
        "    layers.RandomRotation(0.15),\n",
        "    layers.RandomZoom(0.3),\n",
        "    tf.keras.layers.RandomBrightness(0.1),\n",
        "    tf.keras.layers.RandomContrast(0.1)\n",
        "  ]\n",
        ")\n",
        "\n",
        "for images, _ in train_ds.take(2):\n",
        "  for i in range(9):\n",
        "    augmented_images = data_augmentation(images)\n",
        "    ax = plt.subplot(3, 3, i + 1)\n",
        "    plt.imshow(augmented_images[0].numpy().astype(\"uint8\"))\n",
        "    plt.axis(\"off\")"
      ],
      "metadata": {
        "id": "_WkZaXFJjQCt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = tf.keras.Sequential([\n",
        "  data_augmentation, #Data Augmentation\n",
        "  layers.Rescaling(1./255, input_shape=(96, 96, 3)),\n",
        "  layers.Conv2D(32, 3, padding='same', activation='relu'),\n",
        "  layers.MaxPooling2D(),\n",
        "  layers.Conv2D(64, 3, padding='same', activation='relu'),\n",
        "  layers.Conv2D(128, 3, padding='same', activation='relu'),\n",
        "  layers.MaxPooling2D(),\n",
        "  layers.Conv2D(64, 3, padding='same', activation='relu'),\n",
        "  layers.Conv2D(32, 3, padding='same', activation='relu'),\n",
        "\n",
        "  layers.Dropout(0.1), #Regularization\n",
        "  layers.Flatten(),\n",
        "  layers.Dense(512, activation='relu'),\n",
        "  layers.Dense(128, activation='relu'),\n",
        "  layers.Dense(num_classes, activation='softmax')\n",
        "])\n",
        "\n",
        "\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "6p2cmQ37knbW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "epochs_ = 10\n",
        "model.compile(optimizer=\"Adam\", loss=\"sparse_categorical_crossentropy\", metrics=[\"acc\"])\n",
        "history = model.fit(train_ds, batch_size=batch_size_, epochs=epochs_, validation_data=val_ds)"
      ],
      "metadata": {
        "id": "-gDbr2hLlKoQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_history(history)"
      ],
      "metadata": {
        "id": "3ae_hFJXl3LZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Transfer Learning"
      ],
      "metadata": {
        "id": "rUWa7K38logu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "conv_layers =  tf.keras.applications.resnet50.ResNet50(weights='imagenet', include_top=False,\n",
        "                                                        input_tensor=tf.keras.layers.Input(input_shape_),\n",
        "                                                      classes=num_classes)\n",
        "conv_layers.trainable = False\n",
        "\n",
        "\n",
        "conv_layers.summary()\n"
      ],
      "metadata": {
        "id": "c5kh4tRjlnJO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = tf.keras.Sequential([\n",
        "  layers.Input(shape=input_shape_),\n",
        "  conv_layers,\n",
        "  layers.GlobalAveragePooling2D(),\n",
        "  layers.Dropout(0.2),\n",
        "\n",
        "  layers.Flatten(),\n",
        "\n",
        "  layers.Dense(256, activation='relu'),\n",
        "  layers.Dense(128, activation='relu'),\n",
        "  layers.Dense(num_classes, activation='softmax')\n",
        "])\n",
        "\n",
        "model.summary()\n",
        "\n",
        "epochs_ = 20\n",
        "model.compile(optimizer=\"Adam\", loss=\"sparse_categorical_crossentropy\", metrics=[\"acc\"])\n",
        "history = model.fit(train_ds, batch_size=batch_size_, epochs=epochs_, validation_data=val_ds)"
      ],
      "metadata": {
        "id": "xHayL0zumxV9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = tf.keras.Sequential([\n",
        "  layers.Input(shape=input_shape_),\n",
        "  conv_layers,\n",
        "  layers.GlobalAveragePooling2D(),\n",
        "  layers.Dropout(0.2),\n",
        "\n",
        "  layers.Flatten(),\n",
        "\n",
        "  layers.Dense(256, activation='relu'),\n",
        "  layers.Dense(128, activation='relu'),\n",
        "  layers.Dense(num_classes, activation='softmax')\n",
        "])\n",
        "\n",
        "model.summary()\n",
        "\n",
        "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath='/content/my_model/model_checkpoint.weights.h5',  # Specify the file to save the weights\n",
        "    save_best_only=True,             # Save only the best model\n",
        "    save_weights_only=True,\n",
        "    monitor='val_acc' ,              # Monitoring validation loss\n",
        "    mode='max',\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "epochs_ = 20\n",
        "model.compile(optimizer=\"Adam\", loss=\"sparse_categorical_crossentropy\", metrics=[\"acc\"])\n",
        "history = model.fit(train_ds, batch_size=batch_size_, epochs=epochs_, validation_data=val_ds, callbacks=[checkpoint_callback])\n"
      ],
      "metadata": {
        "id": "lglamoWDo-Oz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ls -lah /content/my_model/model_checkpoint.weights.h5"
      ],
      "metadata": {
        "id": "3TI_Yk-Scwny"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_ds = tf.keras.utils.image_dataset_from_directory(\n",
        "  f'./Simpsons/Teste/',\n",
        "  seed=seed_number,\n",
        "  image_size=(input_shape_[0], input_shape_[1]),\n",
        "  batch_size=batch_size_)"
      ],
      "metadata": {
        "id": "hR5t94zBrR2g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.evaluate(test_ds)"
      ],
      "metadata": {
        "id": "0s0Ahicurxsk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.load_weights('/content/my_model/model_checkpoint.weights.h5')\n",
        "model.evaluate(test_ds)"
      ],
      "metadata": {
        "id": "Zf9PEh3zr-8e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "lLngUAEMwMDk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Transfer Leaning => Fine Tunning das ultimas 2 camadas de convolução!"
      ],
      "metadata": {
        "id": "IBajmCNewGG6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "conv_layers.trainable=True\n",
        "\n",
        "n_layers = len(conv_layers.layers)\n",
        "print(n_layers)\n",
        "for j in range(0,n_layers-7):\n",
        "    conv_layers.layers[j].trainable = False\n",
        "\n",
        "\n",
        "\n",
        "for j in range(n_layers-7,n_layers):\n",
        "    conv_layers.layers[j].trainable = True\n",
        "\n",
        "conv_layers.summary()\n",
        "#base_model.layers[j].trainable = True"
      ],
      "metadata": {
        "id": "GlWisBE1sgdm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = tf.keras.Sequential([\n",
        "  layers.Input(shape=input_shape_),\n",
        "\n",
        "  conv_layers,\n",
        "  layers.GlobalAveragePooling2D(),\n",
        "  layers.Dropout(0.3),\n",
        "\n",
        "  layers.Flatten(),\n",
        "  #layers.Dense(512, activation='relu'),\n",
        "  layers.Dense(256, activation='relu'),\n",
        "  layers.Dense(128, activation='relu'),\n",
        "  layers.Dense(num_classes, activation='softmax')\n",
        "])\n",
        "\n",
        "model.summary()\n",
        "\n",
        "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath='/content/my_model/model_checkpoint.weights.h5',  # Specify the file to save the weights\n",
        "    save_best_only=True,             # Save only the best model\n",
        "    save_weights_only=True,\n",
        "    monitor='val_acc' ,              # Monitoring validation loss\n",
        "    mode='max',\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "epochs_ = 20\n",
        "model.compile(optimizer=\"Adam\", loss=\"sparse_categorical_crossentropy\", metrics=[\"acc\"])\n",
        "history = model.fit(train_ds, batch_size=batch_size_, epochs=epochs_, validation_data=val_ds, callbacks=[checkpoint_callback])\n"
      ],
      "metadata": {
        "id": "s2CetdRgwNat"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "for img, lbl in test_ds.take(1):\n",
        "  print(lbl)\n",
        "  prediction = model.predict(img)\n",
        "  predicted_labels = np.argmax(prediction, axis=1)\n",
        "  titles = []\n",
        "  for t,p in zip(lbl,predicted_labels):\n",
        "    titles.append(f'Pred: {class_names[p]} ({class_names[t]})')\n",
        "\n",
        "  plot_sidebyside(img[:5] / 255., titles[:5])\n",
        "\n"
      ],
      "metadata": {
        "id": "T7QeLi0un0kX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "import seaborn as sns\n",
        "\n",
        "y_true = []\n",
        "y_pred = []\n",
        "\n",
        "for images, labels in test_ds:\n",
        "  predictions = model.predict(images)\n",
        "  predicted_labels = np.argmax(predictions, axis=1)\n",
        "  y_true.extend(labels.numpy())\n",
        "  y_pred.extend(predicted_labels)\n",
        "\n",
        "cm = confusion_matrix(y_true, y_pred)\n",
        "# Plot the confusion matrix using Seaborn\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=class_names, yticklabels=class_names)\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('True')\n",
        "plt.title('Confusion Matrix')\n",
        "plt.show()\n",
        "\n",
        "# Generate and print the classification report\n",
        "report = classification_report(y_true, y_pred, target_names=class_names)\n",
        "print(report)\n"
      ],
      "metadata": {
        "id": "BvU84vbbpPDb"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}